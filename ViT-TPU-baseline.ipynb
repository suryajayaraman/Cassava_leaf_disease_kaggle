{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026209,
     "end_time": "2021-02-08T01:28:30.593779",
     "exception": false,
     "start_time": "2021-02-08T01:28:30.567570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023682,
     "end_time": "2021-02-08T01:28:30.642068",
     "exception": false,
     "start_time": "2021-02-08T01:28:30.618386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### install pytorch xla to use TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:28:30.712195Z",
     "iopub.status.busy": "2021-02-08T01:28:30.697863Z",
     "iopub.status.idle": "2021-02-08T01:29:45.475547Z",
     "shell.execute_reply": "2021-02-08T01:29:45.474685Z"
    },
    "papermill": {
     "duration": 74.809612,
     "end_time": "2021-02-08T01:29:45.475768",
     "exception": false,
     "start_time": "2021-02-08T01:28:30.666156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  5116  100  5116    0     0  21677      0 --:--:-- --:--:-- --:--:-- 21677\r\n",
      "Updating... This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-1.7 ...\r\n",
      "Found existing installation: torch 1.7.0\r\n",
      "Uninstalling torch-1.7.0:\r\n",
      "Done updating TPU runtime\r\n",
      "  Successfully uninstalled torch-1.7.0\r\n",
      "Found existing installation: torchvision 0.8.1\r\n",
      "Uninstalling torchvision-0.8.1:\r\n",
      "  Successfully uninstalled torchvision-0.8.1\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/114.2 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/127.4 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/3.1 MiB.                                      \r\n",
      "Processing ./torch-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (0.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (1.19.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (0.18.2)\r\n",
      "Installing collected packages: torch\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.2.5 requires torchvision<0.9,>=0.8, which is not installed.\r\n",
      "fastai 2.2.5 requires torch<1.8,>=1.7.0, but you have torch 1.7.0a0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed torch-1.7.0a0\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.7\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torchvision-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (1.7.0a0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (1.19.5)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (7.2.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (0.18.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (0.6)\r\n",
      "Installing collected packages: torchvision\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.2.5 requires torch<1.8,>=1.7.0, but you have torch 1.7.0a0 which is incompatible.\r\n",
      "fastai 2.2.5 requires torchvision<0.9,>=0.8, but you have torchvision 0.9.0a0+75e4a7d which is incompatible.\u001b[0m\r\n",
      "Successfully installed torchvision-0.9.0a0+75e4a7d\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libomp5\r\n",
      "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\r\n",
      "Need to get 234 kB of archives.\r\n",
      "After this operation, 774 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\r\n",
      "Fetched 234 kB in 1s (283 kB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "(Reading database ... 111506 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-0.3.4-py3-none-any.whl (244 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 244 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.9.0a0+75e4a7d)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0a0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.18.2)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (1.19.5)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.3)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (7.2.0)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.3.4\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version 1.7\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:45.581877Z",
     "iopub.status.busy": "2021-02-08T01:29:45.578057Z",
     "iopub.status.idle": "2021-02-08T01:29:49.881931Z",
     "shell.execute_reply": "2021-02-08T01:29:49.880877Z"
    },
    "papermill": {
     "duration": 4.359248,
     "end_time": "2021-02-08T01:29:49.882107",
     "exception": false,
     "start_time": "2021-02-08T01:29:45.522859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# append package pathss\n",
    "import sys\n",
    "append_paths = ['../input/pytorch-image-models/pytorch-image-models-master', '../input/image-fmix/FMix-master']\n",
    "for package_path in append_paths:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "# basic imports\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# augumentations library\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightnessContrast,ShiftScaleRotate, Cutout, CoarseDropout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, MotionBlur, MedianBlur, GaussianBlur, HueSaturationValue\n",
    "    )\n",
    "import albumentations as A\n",
    "from fmix import sample_mask\n",
    "import cv2\n",
    "\n",
    "# DL library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## pytorch-xla imports\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "# timm import\n",
    "import timm\n",
    "\n",
    "# metrics calculation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# basic plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:49.985877Z",
     "iopub.status.busy": "2021-02-08T01:29:49.984749Z",
     "iopub.status.idle": "2021-02-08T01:29:49.988723Z",
     "shell.execute_reply": "2021-02-08T01:29:49.988178Z"
    },
    "papermill": {
     "duration": 0.056578,
     "end_time": "2021-02-08T01:29:49.988875",
     "exception": false,
     "start_time": "2021-02-08T01:29:49.932297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## For parallelization in TPUs\n",
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047102,
     "end_time": "2021-02-08T01:29:50.083747",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.036645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.207792Z",
     "iopub.status.busy": "2021-02-08T01:29:50.206523Z",
     "iopub.status.idle": "2021-02-08T01:29:50.211608Z",
     "shell.execute_reply": "2021-02-08T01:29:50.212389Z"
    },
    "papermill": {
     "duration": 0.074782,
     "end_time": "2021-02-08T01:29:50.212579",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.137797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # pipeline parameters\n",
    "    SEED        = 42\n",
    "    NUM_CLASSES = 5\n",
    "    TGT_LABEL   = 'label'\n",
    "    TRAIN       = True\n",
    "    LR_FIND     = False\n",
    "    RETRAIN     = False\n",
    "    TEST        = False\n",
    "    DEBUG       = False\n",
    "    N_FOLDS     = 5 \n",
    "    N_EPOCHS    = 5 \n",
    "    DF_FRAC     = 1  \n",
    "    TEST_BATCH_SIZE  = 32\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    SIZE             = [224, 224]\n",
    "    NUM_WORKERS      = 8\n",
    "    FOLD_TO_TRAIN    = [0] #, 1, 2, 3, 4\n",
    "\n",
    "    # model parameters\n",
    "    MODEL_ARCH  = 'vit_base_patch16_224'\n",
    "    MODEL_NAME  = 'vit_v1'\n",
    "    WGT_PATH    = ''\n",
    "    WGT_MODEL   = ''\n",
    "\n",
    "    # loss fn parameters\n",
    "    LOSS_FN     = 'CrossEntropyLoss' # 'LabelSmoothingCrossEntropy'\n",
    "    SMOOTHING   = 0.3\n",
    "    MIX_PROB    = 0.25\n",
    "    \n",
    "    # scheduler variables\n",
    "    SCHEDULER = 'OneCycleLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'CosineAnnealingWarmRestarts']\n",
    "    T_0       = 10 # CosineAnnealingWarmRestarts\n",
    "    MAX_LR    = 3e-4\n",
    "    MIN_LR    = 1e-6\n",
    "    T_MAX     = 5\n",
    "\n",
    "    # optimizer variables\n",
    "    OPTIMIZER     = 'Adam'\n",
    "    WEIGHT_DECAY  = 1e-6\n",
    "    GRD_ACC_STEPS = 1\n",
    "    MAX_GRD_NORM  = 1000\n",
    "    \n",
    "    # Vit parameter\n",
    "    GAMMA = 0.7\n",
    "\n",
    "\n",
    "TRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\n",
    "TEST_PATH = '../input/cassava-leaf-disease-classification/test_images'\n",
    "NPY_FOLDER = '../input/cassava-npy-train-images/train_npy_images'\n",
    "DIR_INPUT = '../input/cassava-leaf-disease-classification'\n",
    "MODEL_PATH = '../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth'\n",
    "\n",
    "index_label_map = {\n",
    "                0: \"Cassava Bacterial Blight (CBB)\", \n",
    "                1: \"Cassava Brown Streak Disease (CBSD)\",\n",
    "                2: \"Cassava Green Mottle (CGM)\", \n",
    "                3: \"Cassava Mosaic Disease (CMD)\", \n",
    "                4: \"Healthy\"\n",
    "                }\n",
    "\n",
    "class_names = [value for key,value in index_label_map.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046643,
     "end_time": "2021-02-08T01:29:50.310669",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.264026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.420778Z",
     "iopub.status.busy": "2021-02-08T01:29:50.418717Z",
     "iopub.status.idle": "2021-02-08T01:29:50.423622Z",
     "shell.execute_reply": "2021-02-08T01:29:50.422987Z"
    },
    "papermill": {
     "duration": 0.06627,
     "end_time": "2021-02-08T01:29:50.423757",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.357487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_results(train_results):\n",
    "    fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "    colors = [\n",
    "        ('#d32f2f', '#ef5350'),\n",
    "        ('#303f9f', '#5c6bc0'),\n",
    "        ('#00796b', '#26a69a'),\n",
    "        ('#fbc02d', '#ffeb3b'),\n",
    "        ('#5d4037', '#8d6e63'),\n",
    "    ]\n",
    "\n",
    "    for i in range(CFG.N_FOLDS):\n",
    "        data = train_results[train_results['fold'] == i]\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values,\n",
    "                                 y=data['train_loss'].values,\n",
    "                                 mode='lines',\n",
    "                                 visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][0], width=2),\n",
    "                                 name='Train loss - Fold #{}'.format(i)),\n",
    "                     row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'],\n",
    "                                 y=data['valid_loss'].values,\n",
    "                                 mode='lines+markers',\n",
    "                                 visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][1], width=2),\n",
    "                                 name='Valid loss - Fold #{}'.format(i)),\n",
    "                     row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values,\n",
    "                                 y=data['valid_score'].values,\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=colors[i][0], width=2),\n",
    "                                 name='Valid score - Fold #{}'.format(i),\n",
    "                                 showlegend=False),\n",
    "                     row=2, col=1)\n",
    "\n",
    "    fig.update_layout({\n",
    "      \"annotations\": [\n",
    "        {\n",
    "          \"x\": 0.225, \n",
    "          \"y\": 1.0, \n",
    "          \"font\": {\"size\": 16}, \n",
    "          \"text\": \"Train / valid losses\", \n",
    "          \"xref\": \"paper\", \n",
    "          \"yref\": \"paper\", \n",
    "          \"xanchor\": \"center\", \n",
    "          \"yanchor\": \"bottom\", \n",
    "          \"showarrow\": False\n",
    "        }, \n",
    "        {\n",
    "          \"x\": 0.775, \n",
    "          \"y\": 1.0, \n",
    "          \"font\": {\"size\": 16}, \n",
    "          \"text\": \"Validation scores\", \n",
    "          \"xref\": \"paper\", \n",
    "          \"yref\": \"paper\", \n",
    "          \"xanchor\": \"center\", \n",
    "          \"yanchor\": \"bottom\", \n",
    "          \"showarrow\": False\n",
    "        }, \n",
    "      ]\n",
    "    })\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.521493Z",
     "iopub.status.busy": "2021-02-08T01:29:50.520738Z",
     "iopub.status.idle": "2021-02-08T01:29:50.524049Z",
     "shell.execute_reply": "2021-02-08T01:29:50.523365Z"
    },
    "papermill": {
     "duration": 0.054405,
     "end_time": "2021-02-08T01:29:50.524204",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.469799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.627529Z",
     "iopub.status.busy": "2021-02-08T01:29:50.626869Z",
     "iopub.status.idle": "2021-02-08T01:29:50.631801Z",
     "shell.execute_reply": "2021-02-08T01:29:50.631164Z"
    },
    "papermill": {
     "duration": 0.060388,
     "end_time": "2021-02-08T01:29:50.631992",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.571604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "set_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.736696Z",
     "iopub.status.busy": "2021-02-08T01:29:50.735312Z",
     "iopub.status.idle": "2021-02-08T01:29:50.740390Z",
     "shell.execute_reply": "2021-02-08T01:29:50.739619Z"
    },
    "papermill": {
     "duration": 0.061091,
     "end_time": "2021-02-08T01:29:50.740535",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.679444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:50.843750Z",
     "iopub.status.busy": "2021-02-08T01:29:50.843099Z",
     "iopub.status.idle": "2021-02-08T01:29:50.846236Z",
     "shell.execute_reply": "2021-02-08T01:29:50.846700Z"
    },
    "papermill": {
     "duration": 0.058455,
     "end_time": "2021-02-08T01:29:50.846879",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.788424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046397,
     "end_time": "2021-02-08T01:29:50.940339",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.893942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.039825Z",
     "iopub.status.busy": "2021-02-08T01:29:51.039203Z",
     "iopub.status.idle": "2021-02-08T01:29:51.111054Z",
     "shell.execute_reply": "2021-02-08T01:29:51.110525Z"
    },
    "papermill": {
     "duration": 0.124633,
     "end_time": "2021-02-08T01:29:51.111213",
     "exception": false,
     "start_time": "2021-02-08T01:29:50.986580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21397, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(f'{DIR_INPUT}/train.csv')\n",
    "#train_df[['cls0', 'cls1', 'cls2', 'cls3', 'cls4']] = train_labels = pd.get_dummies(train_df.iloc[:, 1])\n",
    "train_df['npy_image_id'] = train_df['image_id'].str.replace('jpg', 'npy')\n",
    "if CFG.DF_FRAC < 1:\n",
    "    train_df = train_df.sample(frac=CFG.DF_FRAC).reset_index(drop=True)\n",
    "train_labels = train_df.iloc[:, 1].values\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "folds = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)\n",
    "\n",
    "if CFG.DEBUG == True:\n",
    "    pass\n",
    "    #folds = train_df.copy()\n",
    "    #for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.TGT_LABEL])):\n",
    "    #    folds.loc[val_index, 'fold'] = int(n)\n",
    "    #folds['fold'] = folds['fold'].astype(int)\n",
    "    #print(folds.groupby(['fold', CFG.TGT_LABEL]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.219164Z",
     "iopub.status.busy": "2021-02-08T01:29:51.218431Z",
     "iopub.status.idle": "2021-02-08T01:29:51.221730Z",
     "shell.execute_reply": "2021-02-08T01:29:51.221114Z"
    },
    "papermill": {
     "duration": 0.062677,
     "end_time": "2021-02-08T01:29:51.221919",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.159242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['npy_image_id'].values\n",
    "        self.labels = df[CFG.TGT_LABEL].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(f'{NPY_FOLDER}/{self.file_names[idx]}')\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TEST_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.324578Z",
     "iopub.status.busy": "2021-02-08T01:29:51.323785Z",
     "iopub.status.idle": "2021-02-08T01:29:51.326915Z",
     "shell.execute_reply": "2021-02-08T01:29:51.326404Z"
    },
    "papermill": {
     "duration": 0.057102,
     "end_time": "2021-02-08T01:29:51.327075",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.269973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.DEBUG == True:\n",
    "    train_dataset = TrainDataset(train_df, transform=None)\n",
    "    for i in range(1):\n",
    "        image, label = train_dataset[i]\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'label: {label}')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05976,
     "end_time": "2021-02-08T01:29:51.436288",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.376528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms for Augumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.573997Z",
     "iopub.status.busy": "2021-02-08T01:29:51.573239Z",
     "iopub.status.idle": "2021-02-08T01:29:51.582611Z",
     "shell.execute_reply": "2021-02-08T01:29:51.582082Z"
    },
    "papermill": {
     "duration": 0.077295,
     "end_time": "2021-02-08T01:29:51.582756",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.505461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    targets = (target, shuffled_target, lam)\n",
    "    return new_data, targets\n",
    "\n",
    "def fmix(device, data, targets, alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n",
    "    lam, mask = sample_mask(alpha, decay_power, shape, max_soft, reformulate)\n",
    "    #mask =torch.tensor(mask, device=device).float()\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "    x1 = torch.from_numpy(mask).to(device)*data\n",
    "    x2 = torch.from_numpy(1-mask).to(device)*shuffled_data\n",
    "    targets=(targets, shuffled_targets, lam)\n",
    "    return (x1+x2), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.692684Z",
     "iopub.status.busy": "2021-02-08T01:29:51.691769Z",
     "iopub.status.idle": "2021-02-08T01:29:51.694884Z",
     "shell.execute_reply": "2021-02-08T01:29:51.694189Z"
    },
    "papermill": {
     "duration": 0.063459,
     "end_time": "2021-02-08T01:29:51.695065",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.631606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_transforms():\n",
    "    train_transforms = Compose([\n",
    "            Resize(height=CFG.SIZE[0], width=CFG.SIZE[1]), #RandomResizedCrop(CFG.size, CFG.size),\n",
    "            Transpose(p=0.3), VerticalFlip(p=0.3), HorizontalFlip(p=0.3), ShiftScaleRotate(p=0.4),\n",
    "            RandomBrightnessContrast(p=0.4), \n",
    "            IAAAdditiveGaussianNoise(p=0.3),  # sharpen, affine transform\n",
    "            OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3)], p=0.3),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.3),\n",
    "            CoarseDropout(p=0.4), Cutout(p=0.4),\n",
    "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0)])\n",
    "            # RandomCrop, IAAAdditiveGaussianNoise, RandomResizedCrop(sz,sz),   \n",
    "            # CLAHE, ImageCompression, MaskDropout, elastictransform\n",
    "            # IAAAffine\n",
    "\n",
    "    val_transforms = Compose([\n",
    "            Resize(height=CFG.SIZE[0], width=CFG.SIZE[1]),\n",
    "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0), ToTensorV2(p=1.0)])\n",
    "\n",
    "    test_transforms = Compose([\n",
    "            Resize(height=CFG.SIZE[0], width=CFG.SIZE[1]),\n",
    "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0), ToTensorV2(p=1.0)])\n",
    "\n",
    "    return {'train_transforms':train_transforms, 'val_transforms':val_transforms, 'test_transform':test_transforms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.798655Z",
     "iopub.status.busy": "2021-02-08T01:29:51.797998Z",
     "iopub.status.idle": "2021-02-08T01:29:51.801218Z",
     "shell.execute_reply": "2021-02-08T01:29:51.800576Z"
    },
    "papermill": {
     "duration": 0.057083,
     "end_time": "2021-02-08T01:29:51.801358",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.744275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.DEBUG == True:\n",
    "    train_dataset = TrainDataset(train_df, transform=generate_transforms()['train_transforms'])\n",
    "    for i in range(1):\n",
    "        image, label = train_dataset[i]\n",
    "        plt.imshow(image[0])\n",
    "        plt.title(f'label: {label}')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046977,
     "end_time": "2021-02-08T01:29:51.895229",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.848252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:51.999001Z",
     "iopub.status.busy": "2021-02-08T01:29:51.997994Z",
     "iopub.status.idle": "2021-02-08T01:29:52.003782Z",
     "shell.execute_reply": "2021-02-08T01:29:52.003213Z"
    },
    "papermill": {
     "duration": 0.061402,
     "end_time": "2021-02-08T01:29:52.003944",
     "exception": false,
     "start_time": "2021-02-08T01:29:51.942542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Vision Transformer Models: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vit_base_patch16_224',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch16_224',\n",
       " 'vit_huge_patch32_384',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s3_224']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Available Vision Transformer Models: \")\n",
    "timm.list_models(\"vit*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.109864Z",
     "iopub.status.busy": "2021-02-08T01:29:52.109017Z",
     "iopub.status.idle": "2021-02-08T01:29:52.112679Z",
     "shell.execute_reply": "2021-02-08T01:29:52.112085Z"
    },
    "papermill": {
     "duration": 0.059793,
     "end_time": "2021-02-08T01:29:52.112811",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.053018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, model_name=CFG.MODEL_ARCH, pretrained=False):\n",
    "        super(ViTBase16, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        #if pretrained == False:\n",
    "        #    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, CFG.NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.218264Z",
     "iopub.status.busy": "2021-02-08T01:29:52.217309Z",
     "iopub.status.idle": "2021-02-08T01:29:52.220576Z",
     "shell.execute_reply": "2021-02-08T01:29:52.220094Z"
    },
    "papermill": {
     "duration": 0.05856,
     "end_time": "2021-02-08T01:29:52.220718",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.162158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.DEBUG == True:\n",
    "    model = ViTBase16(model_name=CFG.MODEL_ARCH, pretrained=False)\n",
    "    train_dataset = TrainDataset(train_df, transform=generate_transforms()['train_transforms'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size= 4, shuffle=True,\n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    for image, label in train_loader:\n",
    "        output = model(image)\n",
    "        print(output)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04771,
     "end_time": "2021-02-08T01:29:52.316708",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.268998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.421257Z",
     "iopub.status.busy": "2021-02-08T01:29:52.420611Z",
     "iopub.status.idle": "2021-02-08T01:29:52.424222Z",
     "shell.execute_reply": "2021-02-08T01:29:52.423607Z"
    },
    "papermill": {
     "duration": 0.059477,
     "end_time": "2021-02-08T01:29:52.424370",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.364893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.527427Z",
     "iopub.status.busy": "2021-02-08T01:29:52.526782Z",
     "iopub.status.idle": "2021-02-08T01:29:52.529476Z",
     "shell.execute_reply": "2021-02-08T01:29:52.529921Z"
    },
    "papermill": {
     "duration": 0.057034,
     "end_time": "2021-02-08T01:29:52.530171",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.473137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loss_fn():\n",
    "    if CFG.LOSS_FN == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=CFG.SMOOTHING)\n",
    "    return criterion\n",
    "\n",
    "criterion = get_loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049279,
     "end_time": "2021-02-08T01:29:52.628167",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.578888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Device as cpu or tpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.730429Z",
     "iopub.status.busy": "2021-02-08T01:29:52.729653Z",
     "iopub.status.idle": "2021-02-08T01:29:52.732991Z",
     "shell.execute_reply": "2021-02-08T01:29:52.732419Z"
    },
    "papermill": {
     "duration": 0.056759,
     "end_time": "2021-02-08T01:29:52.733129",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.676370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#device = xm.xla_device()\n",
    "#print(f\"Device found is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049944,
     "end_time": "2021-02-08T01:29:52.833323",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.783379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:52.940754Z",
     "iopub.status.busy": "2021-02-08T01:29:52.940100Z",
     "iopub.status.idle": "2021-02-08T01:29:52.943125Z",
     "shell.execute_reply": "2021-02-08T01:29:52.942510Z"
    },
    "papermill": {
     "duration": 0.060148,
     "end_time": "2021-02-08T01:29:52.943294",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.883146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    #iplot(fig, show_link=False)\n",
    "    fig.write_html(CFG.MODEL_NAME + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.064084Z",
     "iopub.status.busy": "2021-02-08T01:29:53.063212Z",
     "iopub.status.idle": "2021-02-08T01:29:53.066128Z",
     "shell.execute_reply": "2021-02-08T01:29:53.066569Z"
    },
    "papermill": {
     "duration": 0.071878,
     "end_time": "2021-02-08T01:29:53.066777",
     "exception": false,
     "start_time": "2021-02-08T01:29:52.994899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, optimizer, data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))                \n",
    "        \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            images, labels = next(dataloader_it)\n",
    "            #print(images.shape)\n",
    "        except:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            images, labels = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            y_preds = model(images.float())\n",
    "            loss = criterion(y_preds, labels)\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results \n",
    "                    \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss: {loss.item()},smoothed_loss: {smoothed_loss},lr : {lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.174670Z",
     "iopub.status.busy": "2021-02-08T01:29:53.173775Z",
     "iopub.status.idle": "2021-02-08T01:29:53.177454Z",
     "shell.execute_reply": "2021-02-08T01:29:53.176815Z"
    },
    "papermill": {
     "duration": 0.061599,
     "end_time": "2021-02-08T01:29:53.177594",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.115995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.LR_FIND == True:\n",
    "    # create Dataset\n",
    "    temp_train_dataset = TrainDataset(train_df, transform=generate_transforms()['train_transforms'])\n",
    "    temp_train_dataloader = DataLoader(temp_train_dataset, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "\n",
    "    # create model instance\n",
    "    # load pretrained weight file, if present\n",
    "    if CFG.RETRAIN == True:\n",
    "        i_fold = 0\n",
    "        checkpoint = torch.load(f'{CFG.WGT_PATH}/{CFG.WGT_MODEL}_fold{i_fold}.pth')\n",
    "        model = CustomResNext(model_name=CFG.MODEL_ARCH, pretrained=False)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        print(f'Model loaded for {CFG.WGT_MODEL}_fold{i_fold}')\n",
    "            \n",
    "    else:\n",
    "        model = CustomResNext(model_name=CFG.MODEL_ARCH, pretrained=True)\n",
    "        model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "    lr_finder_results = find_lr(model, optimizer, temp_train_dataloader)\n",
    "    plot_lr_finder_results(lr_finder_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049562,
     "end_time": "2021-02-08T01:29:53.277300",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.227738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## One fold train and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.401750Z",
     "iopub.status.busy": "2021-02-08T01:29:53.385579Z",
     "iopub.status.idle": "2021-02-08T01:29:53.405075Z",
     "shell.execute_reply": "2021-02-08T01:29:53.404424Z"
    },
    "papermill": {
     "duration": 0.078474,
     "end_time": "2021-02-08T01:29:53.405232",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.326758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_fold(i_fold, model, optimizer, scheduler, device, dataloader_train, dataloader_valid):\n",
    "    train_fold_results = []\n",
    "    lr_list = []\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    para_train_loader = pl.ParallelLoader(dataloader_train, [device]).per_device_loader(device)\n",
    "    para_valid_loader = pl.ParallelLoader(dataloader_valid, [device]).per_device_loader(device)\n",
    "\n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        xm.master_print('  Epoch {}/{}'.format(epoch + 1, CFG.N_EPOCHS))\n",
    "        #gc.collect()\n",
    "        \n",
    "        model.train()\n",
    "        tr_loss = 0.0    \n",
    "        # training iterator\n",
    "        tr_iterator = iter(para_train_loader)\n",
    "        #train_progress_bar = tqdm(range(len(para_train_loader)))\n",
    "    \n",
    "        #for idx in train_progress_bar:\n",
    "        for idx in range(len(para_train_loader)):\n",
    "            try:\n",
    "                images, labels = next(tr_iterator)\n",
    "                #print(images.shape)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_train)\n",
    "                images, labels = next(tr_iterator)\n",
    "\n",
    "            images = images.to(device,dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.int64)  \n",
    "            \n",
    "            # adding fmix\n",
    "            mix_decision = np.random.rand()\n",
    "            if mix_decision < CFG.MIX_PROB:\n",
    "                images, labels = fmix(device, images, labels, alpha=1., decay_power=5., shape=(CFG.SIZE[0],CFG.SIZE[1]))\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # Forward pass\n",
    "            y_preds = model(images)            \n",
    "            if mix_decision < CFG.MIX_PROB:\n",
    "                loss = criterion(y_preds, labels[0]) * labels[2] + criterion(y_preds, labels[1]) * (1.0 - labels[2])\n",
    "            else:\n",
    "                loss = criterion(y_preds, labels)\n",
    "                    \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            xm.optimizer_step(optimizer)\n",
    "                    \n",
    "            # lr scheduler\n",
    "            scheduler.step()\n",
    "            lr_list.append(optimizer.state_dict()[\"param_groups\"][0]['lr'])\n",
    "            \n",
    "            #if idx % 20 == 0:\n",
    "            #    xm.master_print(f\"Batch {idx} : Train_loss: {tr_loss} loss(avg): {tr_loss/(idx+1)}\")\n",
    "            #train_progress_bar.set_description(f\"Train_loss: {tr_loss} loss(avg): {tr_loss/(idx+1)}\")\n",
    "        #gc.collect()\n",
    "        xm.master_print(f\"Epoch {epoch} : Train_loss: {tr_loss} loss(avg): {tr_loss/(idx+1)}\")\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = None\n",
    "        val_labels = None\n",
    "        valid_iterator = iter(para_valid_loader)\n",
    "        #valid_progress_bar = tqdm(range(len(para_valid_loader)))\n",
    "        #pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "\n",
    "        for idx in range(len(para_valid_loader)):\n",
    "        #for idx in valid_progress_bar:\n",
    "            try:\n",
    "                images, labels = next(valid_iterator)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_valid)\n",
    "                images, labels = next(valid_iterator)\n",
    "            \n",
    "            images = images.to(device,dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.int64)  \n",
    "\n",
    "            if val_labels is None:\n",
    "                val_labels = labels.clone()\n",
    "            else:\n",
    "                val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "            \n",
    "            # computing validation loss and metric\n",
    "            loss = criterion(y_preds, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.softmax(y_preds, dim=1)\n",
    "            \n",
    "            # store predictions            \n",
    "            if val_preds is None:\n",
    "                val_preds = preds\n",
    "            else:\n",
    "                val_preds = torch.cat((val_preds, preds), dim=0)\n",
    "                \n",
    "            # print to console\n",
    "            #valid_progress_bar.set_description(f\"val_loss: {val_loss} loss(avg): {val_loss/(idx+1)}\")\n",
    "        \n",
    "        # save predictions\n",
    "        val_preds  = np.argmax(val_preds.cpu().data.numpy(), axis=1)\n",
    "        val_labels = val_labels.cpu().data.numpy()\n",
    "\n",
    "        # compute accuracy\n",
    "        val_score = accuracy_score(val_labels, val_preds)\n",
    "        cm = confusion_matrix(val_labels, val_preds)\n",
    "        class_wise_acc = []\n",
    "        for i, val in enumerate(cm):\n",
    "            class_wise_acc.append(val[i]/sum(val)*100)\n",
    "        xm.master_print(f\"Fold:{i_fold}, Epoch:{epoch}, val acc:{val_score * 100.0}, Classwise_acc:{class_wise_acc}\")\n",
    "        #gc.collect()\n",
    "        \n",
    "        # store results\n",
    "        train_fold_results.append({ 'fold': i_fold, 'epoch': epoch, 'train_loss': tr_loss / len(dataloader_train), \n",
    "                                    'valid_loss': val_loss / len(dataloader_valid), 'valid_score': val_score,\n",
    "                                    'class_wise_acc': class_wise_acc})\n",
    "            \n",
    "        # save best models        \n",
    "        if val_score > best_val_acc:\n",
    "            # reset variables\n",
    "            best_val_acc = val_score\n",
    "            best_epoch = epoch\n",
    "                        \n",
    "            # save model weights\n",
    "            xm.save({'model': model.state_dict(), 'val_preds':val_preds, 'val_labels':val_labels}, \n",
    "                        f\"{CFG.MODEL_NAME}_fold_{i_fold}_epoch{epoch}_{val_score}.pth\")\n",
    "    \n",
    "    xm.master_print(f\"For Fold {i_fold}, Best validation accuracy of {best_val_acc} was got at epoch {best_epoch}\")                \n",
    "    lr_list = np.array(lr_list)\n",
    "    np.save(f\"{CFG.MODEL_NAME}_fold{i_fold}_LRlist.npy\", lr_list)\n",
    "    return train_fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048782,
     "end_time": "2021-02-08T01:29:53.503215",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.454433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and validation function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.610554Z",
     "iopub.status.busy": "2021-02-08T01:29:53.609875Z",
     "iopub.status.idle": "2021-02-08T01:29:53.612488Z",
     "shell.execute_reply": "2021-02-08T01:29:53.611913Z"
    },
    "papermill": {
     "duration": 0.060275,
     "end_time": "2021-02-08T01:29:53.612628",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.552353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_TPU_Dataloaders(train_data, valid_data):\n",
    "    dataset_train = TrainDataset(train_data, transform=generate_transforms()['train_transforms'])\n",
    "    dataset_valid = TrainDataset(valid_data, transform=generate_transforms()['val_transforms'])\n",
    "            \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset_train,\n",
    "                                    num_replicas=xm.xrt_world_size(),rank=xm.get_ordinal(),shuffle=True)\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(dataset_valid,\n",
    "                                    num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(),shuffle=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset=dataset_train, batch_size=CFG.TRAIN_BATCH_SIZE, sampler=train_sampler,\n",
    "                              drop_last=False, num_workers=CFG.NUM_WORKERS) #pin_memory=False\n",
    "\n",
    "    dataloader_valid = DataLoader(dataset=dataset_valid, batch_size=CFG.TRAIN_BATCH_SIZE, sampler=valid_sampler,\n",
    "                              drop_last=False, num_workers=CFG.NUM_WORKERS) #pin_memory=False\n",
    "    return dataloader_train, dataloader_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.715220Z",
     "iopub.status.busy": "2021-02-08T01:29:53.714552Z",
     "iopub.status.idle": "2021-02-08T01:29:53.727944Z",
     "shell.execute_reply": "2021-02-08T01:29:53.728516Z"
    },
    "papermill": {
     "duration": 0.066416,
     "end_time": "2021-02-08T01:29:53.728711",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.662295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_TPU():\n",
    "    train_results = []\n",
    "    device = xm.xla_device()\n",
    "    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n",
    "    \n",
    "    for i_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_labels)):\n",
    "        if i_fold in CFG.FOLD_TO_TRAIN:\n",
    "            xm.master_print(\"Fold {}/{}\".format(i_fold + 1, CFG.N_FOLDS))\n",
    "\n",
    "            # create fold data\n",
    "            train_data = train_df.iloc[train_idx].reset_index()    \n",
    "            valid_data = train_df.iloc[valid_idx].reset_index()\n",
    "            xm.master_print(train_data.shape, valid_data.shape)\n",
    "            dataloader_train, dataloader_valid = get_TPU_Dataloaders(train_data, valid_data)\n",
    "\n",
    "            # load pretrained weight file\n",
    "            if CFG.RETRAIN == True:\n",
    "                checkpoint = torch.load(f'{CFG.WGT_PATH}/{CFG.WGT_MODEL}_fold{i_fold}.pth')\n",
    "                model = ViTBase16(model_name=CFG.MODEL_ARCH, pretrained=False)\n",
    "                model.to(device)\n",
    "                model.load_state_dict(checkpoint['model'])\n",
    "                xm.master_print(f'Model loaded for {CFG.WGT_MODEL}_fold{i_fold}')\n",
    "\n",
    "            else:\n",
    "                model = ViTBase16(model_name=CFG.MODEL_ARCH, pretrained=True)\n",
    "                model.to(device)\n",
    "\n",
    "            ## optimizer function\n",
    "            if CFG.OPTIMIZER == 'Adam':\n",
    "                optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "            else:\n",
    "                optimizer = optim.SGD(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR, momentum=0.9)\n",
    "\n",
    "\n",
    "            # lr scheduler\n",
    "            if CFG.SCHEDULER == 'OneCycleLR':\n",
    "                scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= CFG.MAX_LR, epochs = CFG.N_EPOCHS, \n",
    "                                                          steps_per_epoch = len(dataloader_train), pct_start=0.4, \n",
    "                                                          div_factor=10, anneal_strategy='cos')\n",
    "            elif CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "                scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, \n",
    "                                                        eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "            else:\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_MAX,eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "\n",
    "            #xm.master_print(f\"scheduler:{scheduler},optimizer:{optimizer},loss_fn:{criterion}\")\n",
    "            train_fold_results = train_one_fold(i_fold, model, optimizer, scheduler, device,\n",
    "                                                dataloader_train, dataloader_valid)\n",
    "            train_results = train_results + train_fold_results\n",
    "    return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.831616Z",
     "iopub.status.busy": "2021-02-08T01:29:53.830930Z",
     "iopub.status.idle": "2021-02-08T01:29:53.836318Z",
     "shell.execute_reply": "2021-02-08T01:29:53.836790Z"
    },
    "papermill": {
     "duration": 0.058864,
     "end_time": "2021-02-08T01:29:53.836994",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.778130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    train_results = fit_TPU()    \n",
    "    train_results = pd.DataFrame(train_results)\n",
    "    print(train_results)\n",
    "    train_results.to_csv('train_results.csv', index=False)\n",
    "    #best_folds = np.array([train_results[train_results['fold']==x]['valid_score'].max() for x in CFG.FOLD_TO_TRAIN])\n",
    "    #print(f'Overall CV accuracy : {best_folds.mean()}, std: {best_folds.std()}')\n",
    "    #plot_training_results(train_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T01:29:53.942168Z",
     "iopub.status.busy": "2021-02-08T01:29:53.941264Z",
     "iopub.status.idle": "2021-02-08T01:30:11.762525Z",
     "shell.execute_reply": "2021-02-08T01:30:11.761439Z"
    },
    "papermill": {
     "duration": 17.876364,
     "end_time": "2021-02-08T01:30:11.762689",
     "exception": false,
     "start_time": "2021-02-08T01:29:53.886325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINING ON 8 TPU CORES\n",
      "Fold 1/5\n",
      "(17117, 4) (4280, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "Exception in device=TPU:3: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 324, in _start_fn\n",
      "    fn(gindex, *args)\n",
      "  File \"<ipython-input-28-2eef5c61b590>\", line 4, in _mp_fn\n",
      "    train_results = fit_TPU()\n",
      "  File \"<ipython-input-27-0311e0fff9dd>\", line 25, in fit_TPU\n",
      "    model = ViTBase16(model_name=CFG.MODEL_ARCH, pretrained=True)\n",
      "  File \"<ipython-input-17-e12cf6af7f3d>\", line 4, in __init__\n",
      "    self.model = timm.create_model(model_name, pretrained=pretrained)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/timm/models/factory.py\", line 57, in create_model\n",
      "    model = create_fn(**model_args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/timm/models/vision_transformer.py\", line 317, in vit_base_patch16_224\n",
      "    model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/timm/models/helpers.py\", line 98, in load_pretrained\n",
      "    state_dict = model_zoo.load_url(cfg['url'], progress=False, map_location='cpu')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/hub.py\", line 555, in load_state_dict_from_url\n",
      "    download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/hub.py\", line 447, in download_url_to_file\n",
      "    buffer = u.read(8192)\n",
      "  File \"/opt/conda/lib/python3.7/http/client.py\", line 461, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.7/http/client.py\", line 505, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "process 3 terminated with exit code 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 raise Exception(\n\u001b[1;32m    111\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 )\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: process 3 terminated with exit code 17"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FLAGS = {}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.056639,
     "end_time": "2021-02-08T01:30:11.876542",
     "exception": false,
     "start_time": "2021-02-08T01:30:11.819903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.218668,
   "end_time": "2021-02-08T01:30:12.945746",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-08T01:28:24.727078",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
