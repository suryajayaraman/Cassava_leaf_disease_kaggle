{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Custom Dataset classes in pytorch\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "k-Fold validation pytorch\n",
    "--\n",
    "1. https://stackoverflow.com/questions/58996242/cross-validation-for-mnist-dataset-with-pytorch-and-sklearn\n",
    "2. https://discuss.pytorch.org/t/i-need-help-in-this-k-fold-cross-validation-implementation/90705/5\n",
    "3. https://github.com/buomsoo-kim/PyTorch-learners-tutorial/blob/master/PyTorch%20Basics/pytorch-datasets-2.ipynb\n",
    "\n",
    "\n",
    "kFold split sklearn\n",
    "--\n",
    "1. sklearn.model_selection.KFold -  normal ordered splits without any shuffle by default. \n",
    "2. sklearn.model_selection.StratifiedKFold - tries to preserve the distribution of each class in each set\n",
    "3. GroupKFold - ensures the group of data is not repeated in any fold; little complex concept\n",
    "4. RepeatedKFold - repeat kfold n times with different random state each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import math\n",
    "#import time\n",
    "#from skimage import io, transform\n",
    "#from typing import Dict\n",
    "#from pathlib import Path\n",
    "\n",
    "# interactive plot libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from plotly.offline import init_notebook_mode, iplot # download_plotlyjs, plot\n",
    "#import plotly.graph_objs as go\n",
    "#from plotly.subplots import make_subplots\n",
    "#init_notebook_mode(connected=True)\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# sklearn related imports\n",
    "# import skorch #sklearn + pytorch functionalitites\n",
    "from sklearn.model_selection import StratifiedKFold #KFold, \n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#import skorch\n",
    "#from skorch.callbacks import Checkpoint\n",
    "#from skorch.callbacks import Freezer\n",
    "#from skorch.helper import predefined_split\n",
    "#from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cfg = {'train_img_path': \"cassava-leaf-disease-classification/train_images/\",\n",
    "            'train_csv_path': 'cassava-leaf-disease-classification/train.csv',\n",
    "            'train' : True, 'lr_find' : False, 'validate' : True, 'test' : False}\n",
    "\n",
    "model_cfg = {'model_architecture': 'resnet18', 'model_name': 'R18_imagenet',\n",
    "             'init_lr': 1e-4, 'weight_path': '', 'train_epochs':5}\n",
    "\n",
    "train_cfg = {'batch_size': 16, 'shuffle': False, 'num_workers': 4, 'checkpt_every' : 1 }\n",
    "valid_cfg = {'batch_size': 16, 'shuffle': False, 'num_workers': 4, 'validate_every' : 1 }\n",
    "test_cfg  = {'batch_size': 16, 'shuffle': False, 'num_workers': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_label_map = {\n",
    "                0: \"Cassava Bacterial Blight (CBB)\", \n",
    "                1: \"Cassava Brown Streak Disease (CBSD)\",\n",
    "                2: \"Cassava Green Mottle (CGM)\", \n",
    "                3: \"Cassava Mosaic Disease (CMD)\", \n",
    "                4: \"Healthy\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- load images into dataset (Dataset class of pytorch maybe)\n",
    "- split into 5 fold data - scikit learn\n",
    "- simple network -r18, r50 with last layers changed to 5 lables\n",
    "- adam optimizer, lr_finder, cross entropy loss\n",
    "- cv score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    #print(total_trainable_params)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_splits(csv_path, cv_splits=3):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df['label'].values\n",
    "    X = np.zeros(y.shape)\n",
    "    \n",
    "    cv_split_fn = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    cv_split_idx = {}\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_split_fn.split(X,y)):\n",
    "        cv_split_idx['split' + str(idx+1) + '_train'] = train_idx\n",
    "        cv_split_idx['split' + str(idx+1) + '_test']  = test_idx\n",
    "    return cv_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    \"\"\"Cassave leaf disease detection dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, idx_list=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            idx_list (list of ints): select only certain rows from csv \n",
    "        \"\"\"\n",
    "        self.cassava_leaf_disease = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if idx_list != None:\n",
    "            self.cassava_leaf_disease = self.cassava_leaf_disease.iloc[idx_list, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cassava_leaf_disease)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.cassava_leaf_disease.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = np.array(self.cassava_leaf_disease.iloc[idx, 1])\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(300),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassava_dataset = CassavaDataset(csv_file=path_cfg['train_csv_path'], root_dir=path_cfg['train_img_path'], \n",
    "                                 transform=transforms)\n",
    "\n",
    "# split to train and validation sets\n",
    "cv_splits = get_cv_splits(path_cfg['train_csv_path'], cv_splits=3)\n",
    "\n",
    "# Datasets\n",
    "train_data = Subset(cassava_dataset, cv_splits['split1_train'])\n",
    "test_data  = Subset(cassava_dataset, cv_splits['split1_test'])\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(train_data, batch_size=train_cfg['batch_size'],shuffle=train_cfg['shuffle'])\n",
    "validateloader = DataLoader(test_data, batch_size=valid_cfg['batch_size'],shuffle=valid_cfg['shuffle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 66309\n"
     ]
    }
   ],
   "source": [
    "output_features = 5\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 128), nn.ReLU(), \n",
    "                                 nn.Linear(128, output_features), nn.LogSoftmax(dim=1)\n",
    "                                )\n",
    "print('Trainable Parameters :', find_no_of_trainable_params(model))\n",
    "#print(model.model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device, loss fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device);\n",
    "\n",
    "# loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=model_cfg['init_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous weight file\n",
    "if model_cfg['weight_path'] != '':\n",
    "    state_dict = torch.load(model_cfg['weight_path'])\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validate_dataloader):\n",
    "    valid_it = iter(validate_dataloader)\n",
    "    progress_bar = tqdm(range(len(validate_dataloader)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in progress_bar: \n",
    "            try:\n",
    "                inputs, labels = next(valid_it)\n",
    "            except StopIteration:\n",
    "                valid_it = iter(validate_dataloader)\n",
    "                inputs, labels = next(valid_it)\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(inputs)\n",
    "            batch_loss = criterion(logps, labels)\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "    test_loss = test_loss/len(validate_dataloader)\n",
    "    test_accuracy = test_accuracy/len(validate_dataloader)\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.043710708618164 loss(avg): 0.9644037976259608: 100%|██████████| 892/892 [08:07<00:00,  1.83it/s]  \n",
      "100%|██████████| 446/446 [03:53<00:00,  1.91it/s]\n",
      "loss: 0.959989070892334 loss(avg): 0.8108083269654902: 100%|██████████| 892/892 [08:06<00:00,  1.83it/s]  \n",
      "100%|██████████| 446/446 [03:53<00:00,  1.91it/s]\n",
      "loss: 1.3579087257385254 loss(avg): 0.7732734635405476: 100%|██████████| 892/892 [08:04<00:00,  1.84it/s] \n",
      "100%|██████████| 446/446 [03:55<00:00,  1.89it/s]\n",
      "loss: 1.0391736030578613 loss(avg): 0.7529018251350642: 100%|██████████| 892/892 [08:03<00:00,  1.84it/s] \n",
      "100%|██████████| 446/446 [03:52<00:00,  1.92it/s]\n",
      "loss: 0.8170357942581177 loss(avg): 0.7438279592302616: 100%|██████████| 892/892 [08:03<00:00,  1.85it/s] \n",
      "100%|██████████| 446/446 [03:53<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if path_cfg['train'] == True:\n",
    "    results = {}\n",
    "    results['train_losses'] = []\n",
    "    #results['train_accuracy'] = []\n",
    "    results['validate_losses'] = []\n",
    "    results['validate_accuracy'] = []\n",
    "\n",
    "    for epoch in range(model_cfg['train_epochs']):\n",
    "        tr_it = iter(trainloader)\n",
    "        progress_bar = tqdm(range(len(trainloader)))            \n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx in progress_bar:\n",
    "            try:\n",
    "                inputs, labels = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(trainloader)\n",
    "                inputs, labels = next(tr_it)\n",
    "\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # store losses\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print to console\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {running_loss/(batch_idx+1)}\")\n",
    "        \n",
    "        results['train_losses'].append(running_loss/len(trainloader))\n",
    "        \n",
    "        # save weights periodically\n",
    "        if (epoch % train_cfg['checkpt_every'] == 0):\n",
    "            torch.save(model.state_dict(), model_cfg['model_name'] + str(epoch+1) + '_epochs.pth')\n",
    "        \n",
    "        # validate periodically\n",
    "        if (epoch % valid_cfg['validate_every'] == 0):\n",
    "            val_loss, val_accuracy = validate(validateloader)\n",
    "            results['validate_losses'].append(val_loss)\n",
    "            results['validate_accuracy'].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8143060549492259,\n",
       " 0.7539211985709421,\n",
       " 0.7269521639619707,\n",
       " 0.7044892492850265,\n",
       " 0.6967150224163928,\n",
       " 0.7009488511245882]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['validate_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6990018110104206,\n",
       " 0.7246464298445013,\n",
       " 0.7395653673886183,\n",
       " 0.7410421697548152,\n",
       " 0.7464319593168695,\n",
       " 0.7442329251980033]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['validate_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "running_loss = 0\n",
    "print_every = 5\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tr_it = iter(trainloader)\n",
    "progress_bar = tqdm(range(len(trainloader)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx in progress_bar:\n",
    "        try:\n",
    "            inputs, labels = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(trainloader)\n",
    "            inputs, labels = next(tr_it)\n",
    "            \n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "    \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {running_loss / (i + 1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(lyft_kaggle)",
   "language": "python",
   "name": "lyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
