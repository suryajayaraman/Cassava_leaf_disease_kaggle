{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Custom Dataset classes in pytorch\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "k-Fold validation pytorch\n",
    "--\n",
    "1. https://stackoverflow.com/questions/58996242/cross-validation-for-mnist-dataset-with-pytorch-and-sklearn\n",
    "2. https://discuss.pytorch.org/t/i-need-help-in-this-k-fold-cross-validation-implementation/90705/5\n",
    "3. https://github.com/buomsoo-kim/PyTorch-learners-tutorial/blob/master/PyTorch%20Basics/pytorch-datasets-2.ipynb\n",
    "\n",
    "\n",
    "kFold split sklearn\n",
    "--\n",
    "1. sklearn.model_selection.KFold -  normal ordered splits without any shuffle by default. \n",
    "2. sklearn.model_selection.StratifiedKFold - tries to preserve the distribution of each class in each set\n",
    "3. GroupKFold - ensures the group of data is not repeated in any fold; little complex concept\n",
    "4. RepeatedKFold - repeat kfold n times with different random state each instance\n",
    "\n",
    "\n",
    "Pytorch Lightning \n",
    "--\n",
    "1. https://www.kaggle.com/pytorchlightning/pytorch-on-tpu-with-pytorch-lightning\n",
    "2. https://www.kaggle.com/arroqc/siim-isic-pytorch-lightning-starter-seresnext50/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# common imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "#import time\n",
    "#from skimage import io, transform\n",
    "#from typing import Dict\n",
    "#from pathlib import Path\n",
    "\n",
    "# interactive plot libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.offline import init_notebook_mode, iplot # download_plotlyjs, plot\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# sklearn related imports\n",
    "# import skorch #sklearn + pytorch functionalitites\n",
    "from sklearn.model_selection import StratifiedKFold #KFold, \n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#import skorch\n",
    "#from skorch.callbacks import Checkpoint\n",
    "#from skorch.callbacks import Freezer\n",
    "#from skorch.helper import predefined_split\n",
    "#from skorch import NeuralNetClassifier\n",
    "\n",
    "# lightning imports\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cfg = {'train_img_path': \"cassava-leaf-disease-classification/train_images/\",\n",
    "            'train_csv_path': 'cassava-leaf-disease-classification/train.csv',\n",
    "            'train' : True, 'lr_find' : False, 'validate' : False, 'test' : False}\n",
    "\n",
    "model_cfg = {'model_architecture': 'resnet18', 'model_name': 'R18_imagenet_v1',\n",
    "             'init_lr': 4e-4, 'weight_path': '', 'train_epochs':3}\n",
    "\n",
    "train_cfg = {'batch_size': 16, 'shuffle': False, 'num_workers': 4, 'checkpt_every' : 1 }\n",
    "valid_cfg = {'batch_size': 16, 'shuffle': False, 'num_workers': 4, 'validate_every' : 1 }\n",
    "test_cfg  = {'batch_size': 16, 'shuffle': False, 'num_workers': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_label_map = {\n",
    "                0: \"Cassava Bacterial Blight (CBB)\", \n",
    "                1: \"Cassava Brown Streak Disease (CBSD)\",\n",
    "                2: \"Cassava Green Mottle (CGM)\", \n",
    "                3: \"Cassava Mosaic Disease (CMD)\", \n",
    "                4: \"Healthy\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- load images into dataset (Dataset class of pytorch maybe)\n",
    "- split into 5 fold data - scikit learn\n",
    "- simple network -r18, r50 with last layers changed to 5 lables\n",
    "- adam optimizer, lr_finder, cross entropy loss\n",
    "- cv score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    #print(total_trainable_params)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_splits(csv_path, cv_splits=3):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df['label'].values\n",
    "    X = np.zeros(y.shape)\n",
    "    \n",
    "    cv_split_fn = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    cv_split_idx = {}\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_split_fn.split(X,y)):\n",
    "        cv_split_idx['split' + str(idx+1) + '_train'] = train_idx\n",
    "        cv_split_idx['split' + str(idx+1) + '_test']  = test_idx\n",
    "    return cv_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    \"\"\"Cassave leaf disease detection dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, idx_list=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            idx_list (list of ints): select only certain rows from csv \n",
    "        \"\"\"\n",
    "        self.cassava_leaf_disease = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if idx_list != None:\n",
    "            self.cassava_leaf_disease = self.cassava_leaf_disease.iloc[idx_list, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cassava_leaf_disease)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.cassava_leaf_disease.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = np.array(self.cassava_leaf_disease.iloc[idx, 1])\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(300),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4303133, 0.49675637, 0.3135656], \n",
    "                         [0.2379062, 0.24065569, 0.22874062])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL_Resnet18(pl.LightningModule):\n",
    "    def __init__(self, criterion, optimizer, init_lr):\n",
    "        super(PL_Resnet18, self).__init__()\n",
    "        backbone_model = resnet18(pretrained=True)\n",
    "        backbone_model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 128), nn.ReLU(), \n",
    "                                 nn.Linear(128, output_features), nn.LogSoftmax(dim=1)\n",
    "                                )\n",
    "        self.model = backbone_model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        log_ps = self.forward(x)\n",
    "        train_loss = self.criterion(log_ps, y)\n",
    "        tensorboard_logs = {'train_loss': train_loss}\n",
    "        return {'loss': train_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        log_ps = self.forward(x)\n",
    "        val_loss = self.criterion(log_ps, y)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \"\"\"\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n",
    "    \"\"\"\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        optimizer = self.optimizer(self.model.fc.parameters(), lr=self.init_lr)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= model_cfg[\"init_lr\"],\n",
    "                                                  total_steps=model_cfg[\"train_epochs\"] * len(self.train_loader),\n",
    "                                                  pct_start=0.4, div_factor=10)\n",
    "        return [optimizer], [scheduler] \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        cassava_dataset = CassavaDataset(csv_file=path_cfg['train_csv_path'], root_dir=path_cfg['train_img_path'], \n",
    "                                 transform=transforms)\n",
    "        # split to train and validation sets\n",
    "        cv_splits = get_cv_splits(path_cfg['train_csv_path'], cv_splits=5)\n",
    "        \n",
    "        self.train_data = Subset(cassava_dataset, cv_splits['split1_train'])\n",
    "        self.val_data = Subset(cassava_dataset, cv_splits['split1_test'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.train_loader = DataLoader(self.train_data, batch_size=train_cfg['batch_size'],shuffle=train_cfg['shuffle'],\n",
    "                           num_workers=train_cfg[\"num_workers\"])\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.val_loader = DataLoader(self.val_data, batch_size=valid_cfg['batch_size'],shuffle=valid_cfg['shuffle'],\n",
    "                           num_workers=valid_cfg[\"num_workers\"])\n",
    "        return self.val_loader\n",
    "\n",
    "    \"\"\"\n",
    "    def test_dataloader(self):\n",
    "        loader = DataLoader(self.mnist_test, batch_size=64, num_workers=4)\n",
    "        return loader\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=model_cfg['init_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.OneCycleLR("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1) # max_epochs=3, check_val_every_n_epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset =  17117\n",
      "Len of test dataset =  4280\n"
     ]
    }
   ],
   "source": [
    "cassava_dataset = CassavaDataset(csv_file=path_cfg['train_csv_path'], root_dir=path_cfg['train_img_path'], \n",
    "                                 transform=transforms)\n",
    "\n",
    "# split to train and validation sets\n",
    "cv_splits = get_cv_splits(path_cfg['train_csv_path'], cv_splits=5)\n",
    "\n",
    "# Datasets\n",
    "train_data = Subset(cassava_dataset, cv_splits['split1_train'])\n",
    "test_data  = Subset(cassava_dataset, cv_splits['split1_test'])\n",
    "\n",
    "print('Len of train dataset = ', len(train_data))\n",
    "print('Len of test dataset = ', len(test_data))\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(train_data, batch_size=train_cfg['batch_size'],shuffle=train_cfg['shuffle'])\n",
    "validateloader = DataLoader(test_data, batch_size=valid_cfg['batch_size'],shuffle=valid_cfg['shuffle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 66309\n"
     ]
    }
   ],
   "source": [
    "output_features = 5\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 128), nn.ReLU(), \n",
    "                                 nn.Linear(128, output_features), nn.LogSoftmax(dim=1)\n",
    "                                )\n",
    "print('Trainable Parameters :', find_no_of_trainable_params(model))\n",
    "#print(model.model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device, loss fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lr_find function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T11:59:59.888697Z",
     "iopub.status.busy": "2020-11-23T11:59:59.887931Z",
     "iopub.status.idle": "2020-11-23T11:59:59.891886Z",
     "shell.execute_reply": "2020-11-23T11:59:59.891371Z"
    },
    "papermill": {
     "duration": 0.04629,
     "end_time": "2020-11-23T11:59:59.891989",
     "exception": false,
     "start_time": "2020-11-23T11:59:59.845699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    #iplot(fig, show_link=False)\n",
    "    fig.write_html(model_cfg['model_name'] + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T11:59:59.981838Z",
     "iopub.status.busy": "2020-11-23T11:59:59.974239Z",
     "iopub.status.idle": "2020-11-23T11:59:59.984754Z",
     "shell.execute_reply": "2020-11-23T11:59:59.984222Z"
    },
    "papermill": {
     "duration": 0.058236,
     "end_time": "2020-11-23T11:59:59.984866",
     "exception": false,
     "start_time": "2020-11-23T11:59:59.926630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            inputs, labels = next(dataloader_it)\n",
    "        except StopIteration:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            inputs, labels = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, labels)\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results        \n",
    "        \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss: {loss.item()},smoothed_loss: {smoothed_loss},lr : {lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path_cfg['lr_find'] == True:\n",
    "    lr_finder_results = find_lr(trainloader)\n",
    "    plot_lr_finder_results(lr_finder_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous weight file\n",
    "if model_cfg['weight_path'] != '':\n",
    "    state_dict = torch.load(model_cfg['weight_path'])\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validate_dataloader):\n",
    "    valid_it = iter(validate_dataloader)\n",
    "    progress_bar = tqdm(range(len(validate_dataloader)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in progress_bar: \n",
    "            try:\n",
    "                inputs, labels = next(valid_it)\n",
    "            except StopIteration:\n",
    "                valid_it = iter(validate_dataloader)\n",
    "                inputs, labels = next(valid_it)\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(inputs)\n",
    "            batch_loss = criterion(logps, labels)\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "    test_loss = test_loss/len(validate_dataloader)\n",
    "    test_accuracy = test_accuracy/len(validate_dataloader)\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.102390170097351 loss(avg): 0.9727651359600441: 100%|██████████| 1070/1070 [10:04<00:00,  1.77it/s] \n",
      "100%|██████████| 268/268 [02:27<00:00,  1.82it/s]\n",
      "loss: 0.7382745742797852 loss(avg): 0.7840901069273458: 100%|██████████| 1070/1070 [09:58<00:00,  1.79it/s] \n",
      "100%|██████████| 268/268 [02:25<00:00,  1.84it/s]\n",
      "loss: 0.7612194418907166 loss(avg): 0.7268509004717675: 100%|██████████| 1070/1070 [10:00<00:00,  1.78it/s] \n",
      "100%|██████████| 268/268 [02:25<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_losses: [0.9727651359600441, 0.7840901069273458, 0.7268509004717675]\n",
      "validate_losses: [0.6919309701492538, 0.7325093283582089, 0.7516324626865671]\n",
      "validate_accuracy: [0.7911192013590194, 0.7137122319927857, 0.6822951678464662]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if path_cfg['train'] == True:\n",
    "    results = {}\n",
    "    results['train_losses'] = []\n",
    "    #results['train_accuracy'] = []\n",
    "    results['validate_losses'] = []\n",
    "    results['validate_accuracy'] = []\n",
    "    lr_list = []\n",
    "\n",
    "    for epoch in range(model_cfg['train_epochs']):\n",
    "        tr_it = iter(trainloader)\n",
    "        progress_bar = tqdm(range(len(trainloader)))            \n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx in progress_bar:\n",
    "            try:\n",
    "                inputs, labels = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(trainloader)\n",
    "                inputs, labels = next(tr_it)\n",
    "\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # lr scheduler\n",
    "            scheduler.step()\n",
    "            lr_list.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # store losses\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print to console\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {running_loss/(batch_idx+1)}\")\n",
    "        \n",
    "        results['train_losses'].append(running_loss/len(trainloader))\n",
    "        \n",
    "        # save weights periodically\n",
    "        if (epoch % train_cfg['checkpt_every'] == 0):\n",
    "            torch.save(model.state_dict(), model_cfg['model_name'] + str(epoch+1) + '_epochs.pth')\n",
    "        \n",
    "        # validate periodically\n",
    "        if (epoch % valid_cfg['validate_every'] == 0):\n",
    "            val_loss, val_accuracy = validate(validateloader)\n",
    "            results['validate_losses'].append(val_loss)\n",
    "            results['validate_accuracy'].append(val_accuracy)\n",
    "    \n",
    "    print('train_losses:', results['train_losses'])\n",
    "    print('validate_losses:', results['validate_losses'])\n",
    "    print('validate_accuracy:', results['validate_accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "running_loss = 0\n",
    "print_every = 5\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tr_it = iter(trainloader)\n",
    "progress_bar = tqdm(range(len(trainloader)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx in progress_bar:\n",
    "        try:\n",
    "            inputs, labels = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(trainloader)\n",
    "            inputs, labels = next(tr_it)\n",
    "            \n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "    \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {running_loss / (i + 1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(lyft_kaggle)",
   "language": "python",
   "name": "lyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
